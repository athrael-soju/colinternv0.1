config:
  (): colpali_engine.trainer.colmodel_training.ColModelTrainingConfig

  # === IO ===
  output_dir: !path outputs/colintern-model

  # === Processor ===
  processor:
    (): colpali_engine.utils.transformers_wrappers.AllPurposeWrapper
    class_to_instanciate: !ext colpali_engine.models.ColInternProcessor
    pretrained_model_name_or_path: "OpenGVLab/InternVL3_5-4B-Instruct"
    trust_remote_code: true
    query_prefix: "Query: "
    max_num_visual_tokens: 768

  # === Model ===
  model:
    (): colpali_engine.utils.transformers_wrappers.AllPurposeWrapper
    class_to_instanciate: !ext colpali_engine.models.ColIntern
    pretrained_model_name_or_path: "OpenGVLab/InternVL3_5-4B-Instruct"
    torch_dtype: !ext torch.bfloat16
    trust_remote_code: true
    attn_implementation: "flash_attention_2"
    device_map: "auto"

  # === Training ===
  train_dataset:
    (): colpali_engine.utils.dataset_transformation.load_train_set
  run_eval: false

  loss_func:
    (): colpali_engine.loss.late_interaction_losses.ColbertPairwiseCELoss

  tr_args:
    (): transformers.training_args.TrainingArguments
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 64
    logging_steps: 50
    save_steps: 500
    save_total_limit: 1
    bf16: true
    tf32: true
    remove_unused_columns: false
    ddp_find_unused_parameters: false
    eval_strategy: "no"

  # === LoRA ===
  peft_config:
    (): peft.LoraConfig
    r: 32
    lora_alpha: 32
    lora_dropout: 0.1
    init_lora_weights: "gaussian"
    bias: "none"
    task_type: "FEATURE_EXTRACTION"
    target_modules: '(.*(model|language_model).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$|.*(custom_text_proj|custom_vision_proj).*$)'

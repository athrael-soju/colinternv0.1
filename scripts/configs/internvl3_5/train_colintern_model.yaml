# scripts/configs/internvl3_5/train_colintern_model.yaml
config:
  (): colpali_engine.trainer.colmodel_training.ColModelTrainingConfig

  # === IO ===
  output_dir: !path outputs/colintern-model

  # === Processor ===
  processor:
    (): colpali_engine.utils.transformers_wrappers.AllPurposeWrapper
    class_to_instanciate: !ext colpali_engine.models.ColInternProcessor
    pretrained_model_name_or_path: "OpenGVLab/InternVL3_5-4B-Instruct"
    trust_remote_code: true
    revision: "main"
    query_prefix: "Query: "
    max_num_visual_tokens: 768
    query_augmentation_token: ""
    # ↓↓↓ NEW ↓↓↓
    image_size: 336        # 448 → 336 (tokens/tile: 1024 → 576)
    max_tiles: 4           # 12 → 4 (3× fewer tiles)
    use_thumbnail: true

  # === Model ===
  model:
    (): colpali_engine.utils.transformers_wrappers.AllPurposeWrapper
    class_to_instanciate: !ext colpali_engine.models.ColIntern
    pretrained_model_name_or_path: "OpenGVLab/InternVL3_5-4B-Instruct"
    trust_remote_code: true
    revision: "main"                 # optional pin
    torch_dtype: !ext torch.bfloat16
    attn_implementation: "flash_attention_2"
    device_map: "cuda"
    use_flash_attn: true

  # === Training ===
  train_dataset:
    (): colpali_engine.utils.dataset_transformation.load_train_set
  run_eval: false

  loss_func:
    (): colpali_engine.loss.late_interaction_losses.ColbertPairwiseCELoss

  tr_args:
    (): transformers.training_args.TrainingArguments
    per_device_train_batch_size: 8
    gradient_accumulation_steps: 16
    # optional VRAM saver
    gradient_checkpointing: false
    # gradient_checkpointing_kwargs: { "use_reentrant": false }
    # logging/checkpointing
    logging_steps: 10
    save_steps: 500
    save_total_limit: 1
    # mixed precision / kernels
    bf16: true
    tf32: true
    # dataloader hygiene (WSL-friendly)
    remove_unused_columns: false
    ddp_find_unused_parameters: false
    dataloader_num_workers: 12
    dataloader_pin_memory: true
    eval_strategy: "no"
    torch_compile: false
    optim: "adamw_torch_fused"
    
  # === LoRA ===
  peft_config:
    (): peft.LoraConfig
    r: 32
    lora_alpha: 32
    lora_dropout: 0.1
    init_lora_weights: "gaussian"
    bias: "none"
    task_type: "FEATURE_EXTRACTION"
    target_modules: '(.*(model|language_model).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$|.*(custom_text_proj|custom_vision_proj).*$)'
